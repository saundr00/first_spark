sudo docker volume create first-spark-vol
sudo docker network create --driver bridge first-spark-net

sudo docker run -itd --rm -v first-spark-vol:/mnt/data --network first-spark-net \
  --hostname spark-master \
  -p 7077:7077 -p 4040:4040 -p 8080:8080 \
  --name spark-master spark bash
sudo docker run -itd --rm -v first-spark-vol:/mnt/data --network first-spark-net \
  --hostname spark-worker-1 \
  --name spark-worker-1 spark bash
sudo docker run -itd --rm -v first-spark-vol:/mnt/data --network first-spark-net \
  --hostname spark-worker-2 \
  --name spark-worker-2 spark bash
sudo docker run -itd --rm -v first-spark-vol:/mnt/data --network first-spark-net \
  --hostname --spark-client \
  --name spark-client spark bash

sudo docker stop spark-client spark-worker-1 spark-worker-2 spark-master
sudo docker rm spark-client spark-worker-1 spark-worker-2 spark-master

sudo docker exec -it spark-master bash
sudo docker attach spark-master

sudo docker stop spark-master
sudo docker prune -f



sudo apt-get update
sudo apt-get upgrade -y
sudo apt-get install software-properties-common -y
sudo apt-get install openjdk-18-jdk -y

export ADMIN_USER=saundr00
export HOME_DIR=/home/$ADMIN_USER
echo "export JAVA_HOME=/usr/lib/jvm/java-18-openjdk-arm64" >> ~/.bashrc

sudo add-apt-repository ppa:deadsnakes/ppa -y
sudo DEBIAN_FRONTEND=noninteractive apt-get install python3.11 -y
sudo apt-get install pip -y
sudo apt-get install python3.11-venv -y
sudo python3.11 -m venv python3.11

sudo apt-get install wget
wget https://dlcdn.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
tar xvzf spark-3.4.1-bin-hadoop3.tgz
mv spark-3.4.1-bin-hadoop3/ spark/
rm spark-3.4.1-bin-hadoop3.tgz

echo "export SPARK_HOME=/spark" >> ~/.bashrc
echo "export PATH=\$PATH:\$SPARK_HOME/bin" >> ~/.bashrc
. /python3.11/bin/activate

sudo docker build -f Dockerfile.spark -t spark .
sudo docker build -f Dockerfile.spark-client -t spark-client .
sudo docker build -f Dockerfile.metastore -t spark-metastore .

sudo docker compose -f spark.yml up -d
sudo docker compose -f spark.yml down

pyspark --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension"

docker exec -u 0 -it metastore-standalone bash
docker exec -it metastore-standalone bash

docker run -d -p 9084:9083 --env SERVICE_NAME=metastore \
  --name metastore-standalone --network spark-net \
  apache/hive:3.1.3

docker run --rm -d -p 9083:9083 --env SERVICE_NAME=metastore \
  --env POSTGRES_USER=postgres --env POSTGRES_PASSWORD=postgres \
  --name spark-metastore-1 -v ./db_init:/docker-entrypoint-initdb.d\
  spark-metastore

docker cp postgresql-42.6.0.jar metastore-standalone:/lib/

schematool -dbType postgres -driver org.postgresql.Driver \
  -url jdbc:postgresql://postgres-spark-metastore:5432/metastore_db \
  -userName hive -passWord hive -initSchema

./schematool -dbType postgres -driver org.postgresql.Driver \
-url jdbc:postgresql://localhost:5432/metastore_db \
-userName postgres -passWord postgres -initSchema

docker buildx build \
  --push \
  --platform linux/arm64,linux/amd64 \
  -t username/custom-nginx:v1 .

pg_dump -U postgres --schema-only metastore_db > $HOME/metastore_db.sql
docker cp spark-metastore-1:/root/metastore_db.sql .
