sudo docker volume create first-spark-vol
sudo docker network create --driver bridge first-spark-net

sudo docker run -itd --rm -v first-spark-vol:/mnt/data --network first-spark-net \
  --hostname spark-master \
  -p 7077:7077 -p 4040:4040 -p 8080:8080 \
  --name spark-master spark bash
sudo docker run -itd --rm -v first-spark-vol:/mnt/data --network first-spark-net \
  --hostname spark-worker-1 \
  --name spark-worker-1 spark bash
sudo docker run -itd --rm -v first-spark-vol:/mnt/data --network first-spark-net \
  --hostname spark-worker-2 \
  --name spark-worker-2 spark bash
sudo docker run -itd --rm -v first-spark-vol:/mnt/data --network first-spark-net \
  --hostname --spark-client \
  --name spark-client spark bash

sudo docker stop spark-client spark-worker-1 spark-worker-2 spark-master
sudo docker rm spark-client spark-worker-1 spark-worker-2 spark-master

sudo docker exec -it spark-master bash
sudo docker attach spark-master

sudo docker stop spark-master
sudo docker prune -f



sudo apt-get update
sudo apt-get upgrade -y
sudo apt-get install software-properties-common -y
sudo apt-get install openjdk-18-jdk -y

export ADMIN_USER=saundr00
export HOME_DIR=/home/$ADMIN_USER
echo "export JAVA_HOME=/usr/lib/jvm/java-18-openjdk-arm64" >> ~/.bashrc

sudo add-apt-repository ppa:deadsnakes/ppa -y
sudo DEBIAN_FRONTEND=noninteractive apt-get install python3.11 -y
sudo apt-get install pip -y
sudo apt-get install python3.11-venv -y
sudo python3.11 -m venv python3.11

sudo apt-get install wget
wget https://dlcdn.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
tar xvzf spark-3.4.1-bin-hadoop3.tgz
mv spark-3.4.1-bin-hadoop3/ spark/
rm spark-3.4.1-bin-hadoop3.tgz

echo "export SPARK_HOME=/spark" >> ~/.bashrc
echo "export PATH=\$PATH:\$SPARK_HOME/bin" >> ~/.bashrc
. /python3.11/bin/activate

sudo docker build -f Dockerfile.spark -t spark .
sudo docker build -f Dockerfile.spark-client -t spark-client .
sudo docker build -f Dockerfile.metastore -t spark-metastore .

sudo docker compose -f spark.yml up -d
sudo docker compose -f spark.yml down

pyspark --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension"
http://52.142.49.246:8889/lab?token=679d76f935190f33ba910181a2baf4f64f7a3724efbde89b

docker exec -u 0 -it metastore-standalone bash
docker exec -it metastore-standalone bash

docker run -d -p 9084:9083 --env SERVICE_NAME=metastore \
  --name metastore-standalone --network spark-net \
  apache/hive:3.1.3

docker run --rm -d -p 5432:5432 \
  --env POSTGRES_USER=postgres --env POSTGRES_PASSWORD=postgres \
  --name spark-metastore-1 -v ./db_init:/docker-entrypoint-initdb.d\
  postgres

docker run --rm -d -p 5432:5432 --env SERVICE_NAME=metastore \
  --env POSTGRES_USER=postgres --env POSTGRES_PASSWORD=postgres \
  --name spark-metastore-1 -v ./db_init:/docker-entrypoint-initdb.d \
  spark-metastore

docker cp postgresql-42.6.0.jar metastore-standalone:/lib/

docker buildx create --name mybuilder
docker buildx use mybuilder

docker buildx build \
  --push \
  --platform linux/arm64/v8,linux/amd64 \
  --tag saundr00/spark:v1 \
  --file Dockerfile.spark .

docker buildx build \
  --push \
  --platform linux/arm64/v8,linux/amd64 \
  --tag saundr00/spark-client:v1 \
  --file Dockerfile.spark-client .


docker run --rm -d -p 5432:5432 --env SERVICE_NAME=metastore \
  --env POSTGRES_USER=postgres --env POSTGRES_PASSWORD=postgres \
  --name spark-metastore-standalone \
  spark-metastore
docker exec -it spark-metastore-standalone bash
cd /var/lib/postgres/apache-hive-3.1.2-bin/bin
./schematool -dbType postgres -driver org.postgresql.Driver \
-url jdbc:postgresql://localhost:5432/metastore_db \
-userName postgres -passWord postgres -initSchema
pg_dump -U postgres --create metastore_db > $HOME/metastore_db.sql
exit
docker cp sspark-metastore-standalone:/root/metastore_db.sql ./db_init/init_01.sql
